# introduction
Technology has greatly influenced human history . Deep learning and machine learning have recently emerged as transformative fields, significantly advancing our comprehension of computer capabilities . The primary goal of this objective is to equip machines with the inherent human capacity for perception, reasoning, and practical application of knowledge [8,9].
Object detection is a crucial aspect of computer vision that has various applications in different industries, particularly in the context of ongoing technological advancements. One important application of personal protective equipment (PPE) is to ensure safety and compliance in various environments ]. This paper explores object detection, specifically the use of the You Only Look Once (YOLO) algorithm for detecting safety attire items like hard hats, safety vests, and goggles in images and videos . The ultimate goal is to develop a real-time safety monitoring system that can determine if individuals in hazardous environments comply with the prescribed safety attire protocols. To achieve our objective, we carefully selected and organized a dataset of safety attire images. We use this dataset to train the YOLO algorithm and thoroughly evaluate its performance. Our experimental results confirm that YOLO is effective in detecting safety attire. This highlights its potential as a practical solution for enhancing safety monitoring and compliance in hazardous work environments.
The primary responsibility of a company's health and safety officer is to educate employees about safety protocols and assist them in choosing suitable work attire . Challenges arise when individuals disregard safety regulations and work without appropriate attire, especially when they go unnoticed by health and safety officials . The Department of Occupational Safety and Health Malaysia has reported concerning statistics for workplace fatalities in the manufacturing and construction industries during 2018 and 2019. Moreover, research highlights that human errors, suboptimal practices, and equipment failures are significant factors contributing to these unfortunate incidents.
# Methodology
2.1 Hardware and Equipment
For the creation of the object detection model, this project will harness both a computer and a camera. The computer, equipped with an AMD Ryzen 7 4800H processor, an NVIDIA GeForce GTX 1650 graphics card, and 16 GB of RAM, will serve as the powerhouse. It will execute code and facilitate model training and testing, ensuring the model's robustness. Simultaneously, the camera will play a pivotal role in validating the object detection model, enabling real-time monitoring by seamlessly integrating with the computer's Windows 10 operating system.
2.2 Software
2.2.1 YOLOv5
The most recent iteration of the YOLO object detection model is called YOLOv5. It is depicted in Figure 1. This most recent version of YOLO shares the same architecture as its predecessors 6, but it makes many improvements over those found in the earlier iterations, including a new neural network architecture that makes use of both the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN), as well as a new labeling tool that makes the annotation process easier. Numerous helpful features, like automatic labeling, labeling shortcuts, and programmable hotkeys, are included in this labeling tool.
2.2.2 Roboflow
Roboflow, a versatile cloud-based platform, offers an array of features that are instrumental in the annotation phase of the project. It allows the images to be annotated by a group of people using multiple devices rather than one single device, meaning the work can be split among multiple individuals (Figure 1). It also keeps track of the number of annotations, the class balances, image sizes, and provides annotation heatmaps, all to ensure the user knows what their datasets are doing
<img width="696" height="268" alt="1" src="https://github.com/user-attachments/assets/1b8730a3-7fc5-48b5-8af4-63a922fbb43d" />

Furthermore, Roboflow has the capability to create multiple versions of augmented images, ensuring that the original dataset remains intact, as augmentation is performed at the final step before the dataset is used for training (Figure 2). Additionally, Roboflow can modify an entire class's name during the pre-processing phase, allowing users to either exclude it completely or change the name to suit their needs. In the context of this project, multiple annotators were employed, health checks were used to monitor annotations, and pre-processing tools were utilized to adjust classes as part of feature selection to fine-tune the dataset for training .
Another part of Roboflow that was crucial to the project’s methodology is its ability to export datasets through download codes. Normally, if a user uses cloud-based annotation tools, they will have to download the finished annotation dataset to run the training on the user’s device. However, Roboflow allows the exporting of datasets to be done all in the cloud when paired with Google Colab. Just by copying the download codes and pasting them into Google Colab, the user can have the
<img width="730" height="397" alt="2" src="https://github.com/user-attachments/assets/72b19cbe-8d5a-49cb-a2b1-06bd3190d277" />
!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="eVJtofoI1QS1cNDOGmTQ")
project = rf.workspace("egyptiom-russion-university-uk2ru").project("saftey-m1nag")
version = project.version(1)
dataset = version.download("yolov5")
2.3 Project Flow
The flowchart in Figure 5 illustrates the sequential steps in the project, highlighting the integration of Roboflow and Google Colab as key components. The process starts with data annotation and preprocessing in Roboflow, then it transfers the datasets to Google Colab. Within Google Colab, code development and model training take place, leveraging the platform's cloud-based infrastructure and hardware accelerators. The collaborative aspect of Google Colab enables easy sharing and interaction with peers. This well-defined flowchart visually represents the project's streamlined and efficient workflow.

<img width="461" height="215" alt="3" src="https://github.com/user-attachments/assets/4bc0da9d-382c-4071-81b6-b782dfa3398e" />
2.3.6 Training dataset
Before the training, the runtime was changed to GPU or TPU for the hardware accelerator to improve training speed. Training it with a CPU would have taken hours. Test and regret: 9 hours for 200 epochs. Some of the options that could be added were the learning rate, which by default is 0.001. The project has tested a learning rate of 0.001, 0.01, 0.05, 0.1, and 0.5. Google Drive can be mounted in Colab for easy access to datasets and model files; however, for this project, mounting the Google Drive is done to store model files and results instead of taking them from there. If the training results prove to be insufficient and fine-tuning the data is required, one simply has to copy a new version of a better dataset download code and paste it into an earlier cell, preferably one above the cell containing the training code. No extensive recoding is required when retraining the data. However, because Google Colab is cloud-based software, if the runtime is terminated, any unsaved data will be deleted, so it is important to download any important data before hand or have backups in the cases of video prediction work where it is required to upload a video to Google Colab for it to validate.
<img width="674" height="224" alt="4" src="https://github.com/user-attachments/assets/0a444042-c058-4613-976d-ede840da2269" />
# Results
Feature Engineering
Feature engineering involves the process of enhancing the performance of predictive modeling on a given dataset through the manipulation and transformation of its feature space. The current approaches for automating this process involve either expanding the dataset explicitly with all transformed features and then performing feature selection, or exploring the transformed feature space through evaluation-guided search. We will utilize a method known as feature selection to identify and define two closely interconnected entities.
The feature selection that was chosen was the exclusion of helmets and gloves, as these variables are highly correlated with helmets and gloves (Figure 5). This aids the model in establishing the criteria for classifying an object as a glove or not a glove, and vice versa. The exclusion of safety shoes from the feature selection can be attributed to the highly improbable scenario of individuals engaging in work activities without any form of footwear.


